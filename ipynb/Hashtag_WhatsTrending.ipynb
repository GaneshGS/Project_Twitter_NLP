{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import random\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2 as pg2\n",
    "import psycopg2.extras as pgex\n",
    "this_host='34.211.59.66'\n",
    "this_user='postgres'\n",
    "this_password='postgres'\n",
    "conn = pg2.connect(host = this_host, \n",
    "                        user = this_user,\n",
    "                        password = this_password)\n",
    "\n",
    "sql_select = '''select created_at, location, tweet_content, cleaned_tweet, hashtags from tweets where hashtags != 'None';'''\n",
    "\n",
    "cur = conn.cursor(cursor_factory=pgex.RealDictCursor)\n",
    "cur.execute(sql_select)\n",
    "rows = cur.fetchall()\n",
    "conn.close()\n",
    "df = pd.DataFrame(rows)\n",
    "df.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['created_datetime'] = pd.to_datetime(df['created_at'])\n",
    "df['year'] = df.created_datetime.apply(lambda x: x.year)\n",
    "df['month'] = df.created_datetime.apply(lambda x: x.month)\n",
    "df['day'] = df.created_datetime.apply(lambda x: x.day)\n",
    "df['dayofweek'] = df.created_datetime.apply(lambda x: x.dayofweek)\n",
    "df['hour'] = df.created_datetime.apply(lambda x: x.hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): redis in /opt/conda/lib/python3.5/site-packages\n",
      "\u001b[33mYou are using pip version 8.1.2, however version 9.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[b'hashtags_tfidf_fit_transform',\n",
       " b'tweet_SVD_fit',\n",
       " b'tweet_tfidf_fit',\n",
       " b'tweet_tfidf_fit_transform']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "!pip install redis\n",
    "import redis\n",
    "redis_ip = '34.211.59.66'\n",
    "r = redis.StrictRedis(redis_ip)\n",
    "r.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118746"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hastages_series = df['hashtags'][df['hashtags'].isnull() == False]\n",
    "len(hastages_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(min_df = 1)\n",
    "hashtags_countvec_fit = count_vectorizer.fit(hastages_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'hashtags_countvec_fit_transform',\n",
       " b'hashtags_tfidf_fit_transform',\n",
       " b'tweet_SVD_fit',\n",
       " b'tweet_tfidf_fit',\n",
       " b'hashtags_countvec_fit',\n",
       " b'tweet_tfidf_fit_transform']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashtags_countvec = pickle.dumps(hashtags_countvec_fit)\n",
    "r.set('hashtags_countvec_fit', hashtags_countvec)\n",
    "r.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2017-05-31 23:50:46'), Timestamp('2017-06-07 14:24:48'))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(df['created_datetime']), max(df['created_datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min(df['created_datetime']) + timedelta(hours = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_delta = max(df['created_datetime']) - min(df['created_datetime'])\n",
    "time_window = time_delta.components.days*24 + time_delta.components.hours\n",
    "time_lag = timedelta(hours = 12)\n",
    "time_gap = timedelta(hours = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3767\n",
      "(12, 54522)\n"
     ]
    }
   ],
   "source": [
    "start_time = min(df['created_datetime'])\n",
    "start_time = start_time\n",
    "end_time = start_time + time_lag\n",
    "subset = df[((df['created_datetime'] < end_time) & (df['created_datetime'] > start_time))]\n",
    "print(len(subset))\n",
    "hashtag_vec = hashtags_countvec_fit.transform(subset)\n",
    "print(hashtag_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_time = min(df['created_datetime'])\n",
    "for i in range(time_window):\n",
    "    start_time = start_time\n",
    "    end_time = start_time + time_lag\n",
    "    subset = df[((df['created_datetime'] < end_time) & (df['created_datetime'] > start_time))]\n",
    "    hashtags_countvec_fit.transform(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_document_term_matrix(vectorizer, corpus):\n",
    "    document_term_matrix = vectorizer.fit_transform(corpus)\n",
    "    document_term_matrix = pd.DataFrame(document_term_matrix.toarray(),\n",
    "                                        index=corpus,\n",
    "                                        columns=vectorizer.get_feature_names ())\n",
    "    return document_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(min_df = 1)\n",
    "document_term_matrix = create_document_term_matrix(count_vectorizer,hastages_series)\n",
    "document_term_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TFIDF_fit = pickle.loads(r.get('tweet_tfidf_fit'))\n",
    "SVD_fit = pickle.loads(r.get('tweet_SVD_fit'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tweets_event_ab_test(event, n = 200):\n",
    "    event_tweet = df[df['cleaned_tweet'].str.contains((event))]\n",
    "    event_tweet.reset_index(inplace = True)\n",
    "    \n",
    "    A_index = list(range(event_tweet.shape[0]))\n",
    "    random.shuffle(A_index)\n",
    "    \n",
    "    n = n\n",
    "    A1_index = []\n",
    "    for i in range(n):\n",
    "        A1_index.append(A_index.pop())\n",
    "    \n",
    "    A1 = event_tweet.iloc[A1_index,:]\n",
    "    A1_vec = np.array([SVD_fit.transform(TFIDF_fit.transform([i])).ravel() for i in A1['cleaned_tweet']])\n",
    "\n",
    "    A2 = event_tweet.iloc[A_index,:] \n",
    "    #A2.reset_index(inplace = True)\n",
    "    A2_vec = np.array([SVD_fit.transform(TFIDF_fit.transform([i])).ravel() for i in A2['cleaned_tweet']])\n",
    "    non_event_tweet = df[~df['cleaned_tweet'].str.contains(('event'))]\n",
    "    non_event_tweet.reset_index(inplace = True)\n",
    "    \n",
    "    B_index = list(range(non_event_tweet.shape[0]))\n",
    "    random.shuffle(B_index)\n",
    "    n = n\n",
    "    B1_index = []\n",
    "    for i in range(n):\n",
    "        B1_index.append(B_index.pop())\n",
    "    m = A2.shape[0]\n",
    "    B2_index = []\n",
    "    for i in range(m):\n",
    "        B2_index.append(B_index.pop())  \n",
    "    \n",
    "    \n",
    "    B1 = non_event_tweet.iloc[B1_index,:]\n",
    "    #B1.reset_index(inplace = True)\n",
    "    B1_vec = np.array([SVD_fit.transform(TFIDF_fit.transform([i])).ravel() for i in B1['cleaned_tweet']])\n",
    "    \n",
    "    B2 = non_event_tweet.iloc[B2_index,:]\n",
    "    #B2.reset_index(inplace = True)\n",
    "    B2_vec = np.array([SVD_fit.transform(TFIDF_fit.transform([i])).ravel() for i in B2['cleaned_tweet']])\n",
    "    \n",
    "    A1_vec_mean = np.mean(A1_vec, axis=0)\n",
    "    A2_vec_mean = np.mean(A2_vec, axis=0)\n",
    "    B1_vec_mean = np.mean(B1_vec, axis=0)\n",
    "    B2_vec_mean = np.mean(B2_vec, axis=0)\n",
    "    \n",
    "    a1a2 = cosine_similarity(A1_vec_mean.reshape(1,-1),A2_vec_mean.reshape(1,-1))[0][0]\n",
    "    b1b2 = cosine_similarity(B2_vec_mean.reshape(1,-1),B1_vec_mean.reshape(1,-1))[0][0]\n",
    "    a1b1 = cosine_similarity(A1_vec_mean.reshape(1,-1),B1_vec_mean.reshape(1,-1))[0][0]\n",
    "    a2b2 = cosine_similarity(A2_vec_mean.reshape(1,-1),B2_vec_mean.reshape(1,-1))[0][0]\n",
    "   \n",
    "    a2_consim_list = []\n",
    "    for i in range(min(n,m)):  \n",
    "        a2_consim_list.append(cosine_similarity(A2_vec[i].reshape(1,-1),A1_vec_mean.reshape(1,-1))[0][0])\n",
    "    a2_mean = np.mean(np.array(a2_consim_list))\n",
    "    a2_std = np.std(np.array(a2_consim_list))\n",
    "    \n",
    "    b2_consim_list = []\n",
    "    for i in range(min(n,m)):  \n",
    "        b2_consim_list.append(cosine_similarity(B2_vec[i].reshape(1,-1),B1_vec_mean.reshape(1,-1))[0][0])\n",
    "    b2_mean = np.mean(b2_consim_list)\n",
    "    b2_std = np.std(b2_consim_list)\n",
    "    \n",
    "    a1b2_consim_list = []\n",
    "    for i in range(min(n,m)):  \n",
    "        a1b2_consim_list.append((cosine_similarity(B2_vec[i].reshape(1,-1),A1_vec_mean.reshape(1,-1))[0][0]))\n",
    "    a1b2_mean = np.mean(a1b2_consim_list)\n",
    "    a1b2_std = np.std(a1b2_consim_list)\n",
    "    \n",
    "    print('A1|A2: ',a1a2,'\\n'\n",
    "                'B1|B2: ',b1b2, '\\n\\n'\n",
    "                'A1|B1: ',a1b1, '\\n'\n",
    "                'A2|B2: ', a2b2, '\\n\\n'\n",
    "                'Cosine Similarity Mean of A2 to A1', a2_mean, '\\n'\n",
    "                'Cosine Similarity Mean of B2 to B1', b2_mean, '\\n'\n",
    "                'Cosine Similarity Mean of B2 to A1', a1b2_mean, '\\n\\n'\n",
    "                'Cosine Similarity STD of A2 to A1', a2_std, '\\n'\n",
    "                'Cosine Similarity STD of B2 to B1', b2_std, '\\n'\n",
    "                'Cosine Similarity STD of B2 to A1', a1b2_std, '\\n'                 \n",
    "                )\n",
    "    \n",
    "    return a2_consim_list, b2_consim_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a2_consim_list, b2_consim_list = tweets_event_ab_test('paris|climate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(range(len(a2_consim_list)), a2_consim_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(range(len(b2_consim_list)), b2_consim_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def event_tweet_count(event):\n",
    "    event_tweet = df[df['cleaned_tweet'].str.contains((event))]\n",
    "    return len(event_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "event_tweet_count('nbafinal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets_event_ab_test('nbafinal', n = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_event_ab_test('travel ban', n = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Top_tweets_in_b(event, n = 200):\n",
    "    A = df[df['cleaned_tweet'].str.contains((event))]\n",
    "    A.reset_index(inplace = True)\n",
    "    A_vec = np.array([nlp(i).vector for i in A['cleaned_tweet']])\n",
    "    A_vec_mean = np.average(A_vec, axis=0)\n",
    "    \n",
    "    \n",
    "    B = df[~df['cleaned_tweet'].str.contains(('event'))]\n",
    "    B.reset_index(inplace = True)\n",
    "    \n",
    "    B_index = list(range(B.shape[0]))\n",
    "    random.shuffle(B_index)\n",
    "    n = n\n",
    "    B1_index = []\n",
    "    for i in range(n):\n",
    "        B1_index.append(B_index.pop())\n",
    "   \n",
    "        \n",
    "    B1 = B.iloc[B1_index,:]\n",
    "    B1.reset_index(inplace = True)\n",
    "    B1_vec = np.array([nlp(i).vector for i in B1['cleaned_tweet']])\n",
    "    \n",
    "    \n",
    "    consim_twt_list = []\n",
    "    for i in range(n):  \n",
    "        consim_twt_list.append((cosine_similarity(B1_vec[i].reshape(1,-1),A_vec_mean.reshape(1,-1))[0][0], B1['cleaned_tweet'][i]))\n",
    "    pd.options.display.max_colwidth = 200\n",
    "    result = pd.DataFrame(consim_twt_list, columns = ['score','tweet'])\n",
    "    print(result.shape)\n",
    "    result_90 = result[result['score']>.90]\n",
    "    result_90.sort_values('score', axis = 0, ascending = False)\n",
    "    return result_90['tweet'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Top_tweets_in_b('paris|climate', n = 30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "event_tweet_count('nationaldonutday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Top_tweets_in_b('nationaldonutday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_event_ab_test('nationaldonutday', n = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
