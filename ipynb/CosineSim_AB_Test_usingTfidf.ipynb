{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import random\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import psycopg2 as pg2\n",
    "import psycopg2.extras as pgex\n",
    "this_host='34.211.59.66'\n",
    "this_user='postgres'\n",
    "this_password='postgres'\n",
    "conn = pg2.connect(host = this_host, \n",
    "                        user = this_user,\n",
    "                        password = this_password)\n",
    "\n",
    "sql_select = '''select created_at, location, tweet_content, cleaned_tweet, vector from tweets;'''\n",
    "\n",
    "cur = conn.cursor(cursor_factory=pgex.RealDictCursor)\n",
    "cur.execute(sql_select)\n",
    "rows = cur.fetchall()\n",
    "conn.close()\n",
    "df = pd.DataFrame(rows)\n",
    "df.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "!pip install redis\n",
    "import redis\n",
    "redis_ip = '34.211.59.66'\n",
    "r = redis.StrictRedis(redis_ip)\n",
    "tweet_tfidf_model_fit = pickle.loads(r.get('tweet_tfidf_model_fit'))\n",
    "tweet_tfidf_transform = pickle.loads(r.get('tweet_tfidf_transform'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "event_tweet = df[df['cleaned_tweet'].str.contains(('nbafinals'))]\n",
    "event_tweet.reset_index(inplace = True)\n",
    "event_tweet_vec = tweet_tfidf_model_fit.transform(event_tweet.cleaned_tweet.values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(event_tweet.shape, event_tweet_vec.shape, tweet_tfidf_transform.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet_tfidf_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SVD = TruncatedSVD(n_components = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVD_model_fit = SVD.fit(tweet_tfidf_transform)\n",
    "r = redis.StrictRedis(redis_ip)\n",
    "LSA = pickle.dumps(SVD_model_fit)\n",
    "r.set('tweet_SVD_model_fit', LSA)\n",
    "r.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSA = pickle.dumps(SVD_model_fit)\n",
    "r.set('tweet_SVD_model_fit', LSA)\n",
    "r.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x130567 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 4 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_tfidf_transform[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = redis.StrictRedis(redis_ip)\n",
    "SVD_fit = pickle.loads(r.get('tweet_SVD_model_fit'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SVD.f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def perform_LSA(n_components, vectorizer, df):\n",
    "    SVD = TruncatedSVD(n_components)\n",
    "    component_names = [\"component_\"+str(i+1) for i in range(n_components)]\n",
    "    latent_semantic_analysis = pd.DataFrame(SVD.fit_transform(df),\n",
    "                                            index = df.index,\n",
    "                                            columns = component_names)\n",
    "    vocabulary_expression = pd.DataFrame(SVD.components_,\n",
    "                                         index = component_names,\n",
    "                                         columns = vectorizer.get_feature_names())\n",
    "    return latent_semantic_analysis, vocabulary_expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tweets_event_ab_test(event, n = 200):\n",
    "    event_tweet = df[df['cleaned_tweet'].str.contains((event))]\n",
    "    event_tweet.reset_index(inplace = True)\n",
    "    \n",
    "    A_index = list(range(event_tweet.shape[0]))\n",
    "    random.shuffle(A_index)\n",
    "    \n",
    "    n = n\n",
    "    A1_index = []\n",
    "    for i in range(n):\n",
    "        A1_index.append(A_index.pop())\n",
    "    \n",
    "    A1 = event_tweet.iloc[A1_index,:]\n",
    "    A1.reset_index(inplace = True)\n",
    "    A1_vec = np.array([tweet_tfidf_model.transform(i).vector for i in A1['cleaned_tweet']])\n",
    "    \n",
    "    A2 = event_tweet.iloc[A_index,:] \n",
    "    A2.reset_index(inplace = True)\n",
    "    A2_vec = np.array([nlp(i).vector for i in A2['cleaned_tweet']])\n",
    "    non_event_tweet = df[~df['cleaned_tweet'].str.contains(('event'))]\n",
    "    non_event_tweet.reset_index(inplace = True)\n",
    "    \n",
    "    B_index = list(range(non_event_tweet.shape[0]))\n",
    "    random.shuffle(B_index)\n",
    "    n = n\n",
    "    B1_index = []\n",
    "    for i in range(n):\n",
    "        B1_index.append(B_index.pop())\n",
    "    m = A2.shape[0]\n",
    "    B2_index = []\n",
    "    for i in range(m):\n",
    "        B2_index.append(B_index.pop())  \n",
    "    \n",
    "    \n",
    "    B1 = non_event_tweet.iloc[B1_index,:]\n",
    "    B1.reset_index(inplace = True)\n",
    "    B1_vec = np.array([nlp(i).vector for i in B1['cleaned_tweet']])\n",
    "    \n",
    "    B2 = non_event_tweet.iloc[B2_index,:]\n",
    "    B2.reset_index(inplace = True)\n",
    "    B2_vec = np.array([nlp(i).vector for i in B2['cleaned_tweet']])\n",
    "    \n",
    "    A1_vec_mean = np.average(A1_vec, axis=0)\n",
    "    A2_vec_mean = np.average(A2_vec, axis=0)\n",
    "    B1_vec_mean = np.average(B1_vec, axis=0)\n",
    "    B2_vec_mean = np.average(B2_vec, axis=0)\n",
    "    \n",
    "    a1a2 = cosine_similarity(A1_vec_mean.reshape(1,-1),A2_vec_mean.reshape(1,-1))[0][0]\n",
    "    b1b2 = cosine_similarity(B2_vec_mean.reshape(1,-1),B1_vec_mean.reshape(1,-1))[0][0]\n",
    "    a1b1 = cosine_similarity(A1_vec_mean.reshape(1,-1),B1_vec_mean.reshape(1,-1))[0][0]\n",
    "    a2b2 = cosine_similarity(A2_vec_mean.reshape(1,-1),B2_vec_mean.reshape(1,-1))[0][0]\n",
    "   \n",
    "    a2_consim_list = []\n",
    "    for i in range(min(n,m)):  \n",
    "        a2_consim_list.append(cosine_similarity(A2_vec[i].reshape(1,-1),A1_vec_mean.reshape(1,-1))[0][0])\n",
    "    a2_mean = np.mean(np.array(a2_consim_list))\n",
    "    a2_std = np.std(np.array(a2_consim_list))\n",
    "    \n",
    "    b2_consim_list = []\n",
    "    for i in range(min(n,m)):  \n",
    "        b2_consim_list.append(cosine_similarity(B2_vec[i].reshape(1,-1),B1_vec_mean.reshape(1,-1))[0][0])\n",
    "    b2_mean = np.mean(b2_consim_list)\n",
    "    b2_std = np.std(b2_consim_list)\n",
    "    \n",
    "    a1b2_consim_list = []\n",
    "    for i in range(min(n,m)):  \n",
    "        a1b2_consim_list.append((cosine_similarity(B2_vec[i].reshape(1,-1),A1_vec_mean.reshape(1,-1))[0][0]))\n",
    "    a1b2_mean = np.mean(a1b2_consim_list)\n",
    "    a1b2_std = np.std(a1b2_consim_list)\n",
    "    \n",
    "    print('A1|A2: ',a1a2,'\\n'\n",
    "                'B1|B2: ',b1b2, '\\n\\n'\n",
    "                'A1|B1: ',a1b1, '\\n'\n",
    "                'A2|B2: ', a2b2, '\\n\\n'\n",
    "                'Cosine Similarity Mean of A2 to A1', a2_mean, '\\n'\n",
    "                'Cosine Similarity Mean of B2 to B1', b2_mean, '\\n'\n",
    "                'Cosine Similarity Mean of B2 to A1', a1b2_mean, '\\n\\n'\n",
    "                'Cosine Similarity STD of A2 to A1', a2_std, '\\n'\n",
    "                'Cosine Similarity STD of B2 to B1', b2_std, '\\n'\n",
    "                'Cosine Similarity STD of B2 to A1', a1b2_std, '\\n'                 \n",
    "                )\n",
    "    \n",
    "    return a2_consim_list, b2_consim_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LSA.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a2_consim_list, b2_consim_list = tweets_event_ab_test('paris|climate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(range(len(a2_consim_list)), a2_consim_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(range(len(b2_consim_list)), b2_consim_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def event_tweet_count(event):\n",
    "    event_tweet = df[df['cleaned_tweet'].str.contains((event))]\n",
    "    return len(event_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "event_tweet_count('nbafinal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets_event_ab_test('nbafinal', n = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_event_ab_test('travel ban', n = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Top_tweets_in_b(event, n = 200):\n",
    "    A = df[df['cleaned_tweet'].str.contains((event))]\n",
    "    A.reset_index(inplace = True)\n",
    "    A_vec = np.array([nlp(i).vector for i in A['cleaned_tweet']])\n",
    "    A_vec_mean = np.average(A_vec, axis=0)\n",
    "    \n",
    "    \n",
    "    B = df[~df['cleaned_tweet'].str.contains(('event'))]\n",
    "    B.reset_index(inplace = True)\n",
    "    \n",
    "    B_index = list(range(B.shape[0]))\n",
    "    random.shuffle(B_index)\n",
    "    n = n\n",
    "    B1_index = []\n",
    "    for i in range(n):\n",
    "        B1_index.append(B_index.pop())\n",
    "   \n",
    "        \n",
    "    B1 = B.iloc[B1_index,:]\n",
    "    B1.reset_index(inplace = True)\n",
    "    B1_vec = np.array([nlp(i).vector for i in B1['cleaned_tweet']])\n",
    "    \n",
    "    \n",
    "    consim_twt_list = []\n",
    "    for i in range(n):  \n",
    "        consim_twt_list.append((cosine_similarity(B1_vec[i].reshape(1,-1),A_vec_mean.reshape(1,-1))[0][0], B1['cleaned_tweet'][i]))\n",
    "    pd.options.display.max_colwidth = 200\n",
    "    result = pd.DataFrame(consim_twt_list, columns = ['score','tweet'])\n",
    "    print(result.shape)\n",
    "    result_90 = result[result['score']>.90]\n",
    "    result_90.sort_values('score', axis = 0, ascending = False)\n",
    "    return result_90['tweet'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Top_tweets_in_b('paris|climate', n = 30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "event_tweet_count('nationaldonutday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Top_tweets_in_b('nationaldonutday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_event_ab_test('nationaldonutday', n = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
