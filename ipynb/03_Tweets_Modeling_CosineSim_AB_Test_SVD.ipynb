{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): redis in /opt/conda/lib/python3.5/site-packages\n",
      "\u001b[33mYou are using pip version 8.1.2, however version 9.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied (use --upgrade to upgrade): wordcloud in /opt/conda/lib/python3.5/site-packages\n",
      "Requirement already satisfied (use --upgrade to upgrade): matplotlib in /opt/conda/lib/python3.5/site-packages (from wordcloud)\n",
      "Requirement already satisfied (use --upgrade to upgrade): numpy>=1.6.1 in /opt/conda/lib/python3.5/site-packages (from wordcloud)\n",
      "Requirement already satisfied (use --upgrade to upgrade): pillow in /opt/conda/lib/python3.5/site-packages (from wordcloud)\n",
      "Requirement already satisfied (use --upgrade to upgrade): six>=1.10 in /opt/conda/lib/python3.5/site-packages (from matplotlib->wordcloud)\n",
      "Requirement already satisfied (use --upgrade to upgrade): python-dateutil in /opt/conda/lib/python3.5/site-packages (from matplotlib->wordcloud)\n",
      "Requirement already satisfied (use --upgrade to upgrade): pytz in /opt/conda/lib/python3.5/site-packages (from matplotlib->wordcloud)\n",
      "Requirement already satisfied (use --upgrade to upgrade): cycler>=0.10 in /opt/conda/lib/python3.5/site-packages/cycler-0.10.0-py3.5.egg (from matplotlib->wordcloud)\n",
      "Requirement already satisfied (use --upgrade to upgrade): pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=1.5.6 in /opt/conda/lib/python3.5/site-packages (from matplotlib->wordcloud)\n",
      "Requirement already satisfied (use --upgrade to upgrade): olefile in /opt/conda/lib/python3.5/site-packages (from pillow->wordcloud)\n",
      "\u001b[33mYou are using pip version 8.1.2, however version 9.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install redis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import random\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import math\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "!pip install wordcloud\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import os,sys,inspect\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir)\n",
    "from lib.conn_postgres import connect_to_postgres as conpg,connect_to_redis as conrds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn, cur = conpg(location = 'postgres')\n",
    "sql_select = '''select created_at, location, tweet_content, cleaned_tweet, hashtags from tweets\n",
    "                  where (date_time >= NOW() - '7 day'::INTERVAL);'''\n",
    "\n",
    "cur.execute(sql_select)\n",
    "rows = cur.fetchall()\n",
    "conn.close()\n",
    "df = pd.DataFrame(rows)\n",
    "df.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'hashtag_tfidf',\n",
       " b'a',\n",
       " b'tweets_tfd_svd_pipe',\n",
       " b'tweet_tfidf_fit',\n",
       " b'crackit',\n",
       " b'ktkzfnpuhr',\n",
       " b'xducctihks',\n",
       " b'jtonvogrin',\n",
       " b'tweet_tfidf_fit_transform',\n",
       " b'eaycmfglto',\n",
       " b'hashtags_tfidf_fit_transform',\n",
       " b'tweet_SVD_fit',\n",
       " b'hashtags_countvec_fit',\n",
       " b'lphhavftgd',\n",
       " b'temp_hash_count_12hr',\n",
       " b'hashtags_countvec_fit_temp',\n",
       " b'hashtags_countvec_fit_transform']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = conrds()\n",
    "r.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-fc08b29b83fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtfd_svd_pipe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tweets_tfd_svd_pipe'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "tfd_svd_pipe = pickle.loads(r.get('tweets_tfd_svd_pipe'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TFIDF_fit =  tfd_svd_pipe.steps[0][1]\n",
    "SVD_fit = tfd_svd_pipe.steps[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def event_tweet_count(event):\n",
    "    event = event.lower()\n",
    "    event_tweet = df[df['cleaned_tweet'].str.contains((event))]\n",
    "    return len(event_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tweets_event_ab_test(event):\n",
    "    event = event.lower()\n",
    "    event_tweet = df[df['cleaned_tweet'].str.contains((event))]\n",
    "    event_tweet.reset_index(inplace = True)\n",
    "    \n",
    "    A_index = list(range(event_tweet.shape[0]))\n",
    "    random.shuffle(A_index)\n",
    "    \n",
    "    n = int(round((event_tweet_count(event))*0.2,0))\n",
    "    A1_index = []\n",
    "    for i in range(n):\n",
    "        A1_index.append(A_index.pop())\n",
    "    \n",
    "    A1 = event_tweet.iloc[A1_index,:]\n",
    "    A1_vec = np.array([SVD_fit.transform(TFIDF_fit.transform([i])).ravel() for i in A1['cleaned_tweet']])\n",
    "\n",
    "    A2 = event_tweet.iloc[A_index,:] \n",
    "    #A2.reset_index(inplace = True)\n",
    "    A2_vec = np.array([SVD_fit.transform(TFIDF_fit.transform([i])).ravel() for i in A2['cleaned_tweet']])\n",
    "    non_event_tweet = df[~df['cleaned_tweet'].str.contains(('event'))]\n",
    "    non_event_tweet.reset_index(inplace = True)\n",
    "    \n",
    "    B_index = list(range(non_event_tweet.shape[0]))\n",
    "    random.shuffle(B_index)\n",
    "    n = n\n",
    "    B1_index = []\n",
    "    for i in range(n):\n",
    "        B1_index.append(B_index.pop())\n",
    "    m = A2.shape[0]\n",
    "    B2_index = []\n",
    "    for i in range(m):\n",
    "        B2_index.append(B_index.pop())  \n",
    "    \n",
    "    B1 = non_event_tweet.iloc[B1_index,:]\n",
    "    B1_vec = np.array([SVD_fit.transform(TFIDF_fit.transform([i])).ravel() for i in B1['cleaned_tweet']])\n",
    "    \n",
    "    B2 = non_event_tweet.iloc[B2_index,:]\n",
    "    B2_vec = np.array([SVD_fit.transform(TFIDF_fit.transform([i])).ravel() for i in B2['cleaned_tweet']])\n",
    "    \n",
    "    A1_vec_mean = np.mean(A1_vec, axis=0)\n",
    "    A2_vec_mean = np.mean(A2_vec, axis=0)\n",
    "    B1_vec_mean = np.mean(B1_vec, axis=0)\n",
    "    B2_vec_mean = np.mean(B2_vec, axis=0)\n",
    "    \n",
    "    a1a2 = cosine_similarity(A1_vec_mean.reshape(1,-1),A2_vec_mean.reshape(1,-1))[0][0]\n",
    "    b1b2 = cosine_similarity(B2_vec_mean.reshape(1,-1),B1_vec_mean.reshape(1,-1))[0][0]\n",
    "    a1b1 = cosine_similarity(A1_vec_mean.reshape(1,-1),B1_vec_mean.reshape(1,-1))[0][0]\n",
    "    a2b2 = cosine_similarity(A2_vec_mean.reshape(1,-1),B2_vec_mean.reshape(1,-1))[0][0]\n",
    "   \n",
    "    a2_consim_list = []\n",
    "    for i in range(min(n,m)):  \n",
    "        a2_consim_list.append(cosine_similarity(A2_vec[i].reshape(1,-1),A1_vec_mean.reshape(1,-1))[0][0])\n",
    "    a2_mean = np.mean(np.array(a2_consim_list))\n",
    "    a2_std = np.std(np.array(a2_consim_list))\n",
    "    \n",
    "    b2_consim_list = []\n",
    "    for i in range(min(n,m)):  \n",
    "        b2_consim_list.append(cosine_similarity(B2_vec[i].reshape(1,-1),B1_vec_mean.reshape(1,-1))[0][0])\n",
    "    b2_mean = np.mean(b2_consim_list)\n",
    "    b2_std = np.std(b2_consim_list)\n",
    "    \n",
    "    a1b2_consim_list = []\n",
    "    for i in range(min(n,m)):  \n",
    "        a1b2_consim_list.append((cosine_similarity(B2_vec[i].reshape(1,-1),A1_vec_mean.reshape(1,-1))[0][0]))\n",
    "    a1b2_mean = np.mean(a1b2_consim_list)\n",
    "    a1b2_std = np.std(a1b2_consim_list)\n",
    "    print('Cosine Similarity')\n",
    "    print('\\t[A1_mean|A2_mean] : ',round(a1a2,3),'\\t|\\tmean of [A1_mean|A2] :',round(a2_mean,3), \\\n",
    "                                  '\\t|\\tSTD of [A1_mean|A2] :', round(a2_std,3), '\\n',\n",
    "          '\\t[B1_mean|B2_mean] : ',round(b1b2,3), '\\t|\\tmean of [B1_mean|B2] :', round(b2_mean,3), \\\n",
    "                                   '\\t|\\tSTD of [B1_mean|B2] :', round(b2_std,3), '\\n',\n",
    "          '\\t[A1_mean|B2_mean] : ', round(a2b2,3), '\\t|\\tmean of [A1_mean|B2] :', round(a1b2_mean,3),\\\n",
    "                                    '\\t|\\tSTD of [A1_mean|B2] :', round(a1b2_std,3)) \n",
    "    plt.figure(figsize=(18,6))\n",
    "    ax1 = plt.subplot(121)\n",
    "    ax2 = plt.subplot(122)\n",
    "    ax1.scatter(range(len(a2_consim_list)), a2_consim_list, c = 'g', label = 'cosine similarity of A vectors')\n",
    "    ax1.scatter(range(len(b2_consim_list)), b2_consim_list, c = 'r', label = 'cosine similarity of B vectors')    \n",
    "    sns.distplot(a2_consim_list,color='g',ax = ax2, label = 'cosine similarity of A vectors')\n",
    "    sns.distplot(b2_consim_list,color='r',ax = ax2, label = 'cosine similarity of B vectors')\n",
    "    plt.show()\n",
    "    \n",
    "    component_1 = np.argsort(SVD_fit.explained_variance_ratio_)[::-1][0]\n",
    "    component_2 = np.argsort(SVD_fit.explained_variance_ratio_)[::-1][1]\n",
    "    plt.figure(figsize=(8,6))\n",
    "    \n",
    "   \n",
    "    plt.scatter(B1_vec.T[component_1], B1_vec.T[component_2], facecolor=\"red\", alpha=.7,marker='^', s = 100, label = 'B1 Vector')\n",
    "    plt.scatter(B2_vec.T[component_1], B2_vec.T[component_2], facecolor=\"red\", alpha=0.5,marker='x', s = 100, label = 'B2 Vector')\n",
    "    plt.scatter(A2_vec.T[component_1], A2_vec.T[component_2], facecolor=\"green\", alpha=0.5,marker='x', s = 100, label = 'A2 Vector')\n",
    "    plt.scatter(A1_vec.T[component_1], A1_vec.T[component_2], facecolor=\"green\", alpha=.7,marker='^', s = 100, label = 'A1 Vector')\n",
    "    #plt.xlim(-.2,1)\n",
    "    #plt.ylim(-.2,1)\n",
    "    \n",
    "    # this is another inset axes over the main axes\n",
    "    #a = plt.axes([0.5, 0.5, .45, .45], facecolor='y', alpha=.4)\n",
    "    #plt.scatter(B1_vec.T[component_1], B1_vec.T[component_2], facecolor=\"red\", alpha=.7,marker='^', s = 100)\n",
    "    #plt.scatter(A1_vec.T[component_1], A1_vec.T[component_2], facecolor=\"green\", alpha=.7,marker='^', s = 100)\n",
    "    #plt.scatter(B2_vec.T[component_1], B2_vec.T[component_2], facecolor=\"red\", alpha=0.5,marker='x', s = 100)\n",
    "    #plt.scatter(A2_vec.T[component_1], A2_vec.T[component_2], facecolor=\"green\", alpha=0.5,marker='x', s = 100)\n",
    "    #plt.title('subplot')\n",
    "    #plt.xlim(-.2, 0.2)\n",
    "    #plt.xlim(-.2, 0.2)\n",
    "    #plt.xticks([])\n",
    "    #plt.yticks([])\n",
    "    return a2_consim_list, b2_consim_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_score_cluster(event,check_by_score = .7,high_score = .9):\n",
    "    event = event.lower()\n",
    "    event_tweet = df[df['cleaned_tweet'].str.contains((event))]\n",
    "    event_tweet.reset_index(inplace = True, drop=True)\n",
    "    A_index = list(range(event_tweet.shape[0]))\n",
    "    random.shuffle(A_index)\n",
    "\n",
    "    n = int(round((event_tweet_count(event))*0.5,0))\n",
    "    A1_index = []\n",
    "    for i in range(n):\n",
    "        A1_index.append(A_index.pop())\n",
    "\n",
    "    A1 = event_tweet.iloc[A1_index,:]\n",
    "    A1.reset_index(inplace = True, drop=True)\n",
    "    A1_vec = SVD_fit.transform(TFIDF_fit.transform(A1['cleaned_tweet']))\n",
    "\n",
    "    A2 = event_tweet.iloc[A_index,:] \n",
    "    A2_vec = SVD_fit.transform(TFIDF_fit.transform(A2['cleaned_tweet']))\n",
    "    A1_vec_mean = np.mean(A1_vec, axis=0)\n",
    "    A2_vec_mean = np.mean(A2_vec, axis=0)\n",
    "\n",
    "    consim_twt_list = []\n",
    "    for i in range(len(A1)):  \n",
    "        consim_twt_list.append((cosine_similarity(A1_vec[i].reshape(1,-1),A2_vec_mean.reshape(1,-1))[0][0], A1['cleaned_tweet'][i]))\n",
    "    #pd.options.display.max_colwidth = 200\n",
    "    result = pd.DataFrame(consim_twt_list, columns = ['score','tweet'])\n",
    "    cluster1 = result[result['score']>high_score].sort_values('score', ascending = False)\n",
    "    cluster2 = result[((result['score']<(check_by_score+.05)) & (result['score']>(check_by_score-.05)))]\\\n",
    "                .sort_values('score', ascending = False)\n",
    "    return pd.concat([cluster1.reset_index().head(30), cluster2.reset_index().head(30)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Top_scored_tweets_in_B(event, n = 10000):\n",
    "    event = event.lower()\n",
    "    A = df[df['cleaned_tweet'].str.contains((event))]\n",
    "    A.reset_index(inplace = True, drop=True)\n",
    "    A_vec = SVD_fit.transform(TFIDF_fit.transform(A['cleaned_tweet']))\n",
    "    A_vec_mean = np.average(A_vec, axis=0)\n",
    "       \n",
    "    B = df[~df['cleaned_tweet'].str.contains((event))]\n",
    "    B_index = list(range(B.shape[0]))\n",
    "    random.shuffle(B_index)\n",
    "    n = n\n",
    "    B1_index = []\n",
    "    for i in range(n):\n",
    "        B1_index.append(B_index.pop())\n",
    "        \n",
    "    B1 = B.iloc[B1_index,:]\n",
    "    B1.reset_index(inplace = True, drop=True)\n",
    "    B1_vec = SVD_fit.transform(TFIDF_fit.transform(B1['cleaned_tweet']))\n",
    "    \n",
    "    consim_twt_list = []\n",
    "    for i in range(n):  \n",
    "        consim_twt_list.append((cosine_similarity(B1_vec[i].reshape(1,-1),A_vec_mean.reshape(1,-1))[0][0], B1['cleaned_tweet'][i]))\n",
    "    pd.options.display.max_colwidth = 200\n",
    "    result = pd.DataFrame(consim_twt_list, columns = ['score','tweet'])\n",
    "    cluster = result[result['score']>.7].sort_values('score', ascending = False)\n",
    "    pd.set_option('display.max_colwidth', -1)\n",
    "    return cluster.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = df['hashtags'][(df['hashtags'].isnull() == False) & (df['hashtags'] != 'None')]\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords.add(\"job\")\n",
    "stopwords.add(\"careerarc\")\n",
    "stopwords.add(\"hiring\")\n",
    "wc = WordCloud(width=1600, height=800, background_color='white', \\\n",
    "               relative_scaling=1, stopwords=stopwords, colormap='copper').generate(' '.join(i for i in words))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Event Tweets (earthquake|quake): ', event_tweet_count('earthquake|quake'))\n",
    "A, B = tweets_event_ab_test('earthquake|quake')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check high score tweets (>0.9) versus tweets with clustered scores \n",
    "check_score_cluster('earthquake|quake',.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Top_scored_tweets_in_B('earthquake|quake', n = 300000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
