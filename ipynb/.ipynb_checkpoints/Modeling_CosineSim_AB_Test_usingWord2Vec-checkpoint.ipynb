{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import random\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import psycopg2 as pg2\n",
    "import psycopg2.extras as pgex\n",
    "this_host='34.211.59.66'\n",
    "this_user='postgres'\n",
    "this_password='postgres'\n",
    "conn = pg2.connect(host = this_host, \n",
    "                        user = this_user,\n",
    "                        password = this_password)\n",
    "\n",
    "sql_select = '''select created_at, location, tweet_content, cleaned_tweet, vector from tweets;'''\n",
    "\n",
    "cur = conn.cursor(cursor_factory=pgex.RealDictCursor)\n",
    "cur.execute(sql_select)\n",
    "rows = cur.fetchall()\n",
    "conn.close()\n",
    "df = pd.DataFrame(rows)\n",
    "df.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): redis in /opt/conda/lib/python3.5/site-packages\n",
      "\u001b[33mYou are using pip version 8.1.2, however version 9.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "!pip install redis\n",
    "import redis\n",
    "redis_ip = '34.211.59.66'\n",
    "r = redis.StrictRedis(redis_ip)\n",
    "tweet_tfidf_model_fit = pickle.loads(r.get('tweet_tfidf_model_fit'))\n",
    "tweet_tfidf_transform = pickle.loads(r.get('tweet_tfidf_transform'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "event_tweet = df[df['cleaned_tweet'].str.contains(('nbafinals'))]\n",
    "event_tweet.reset_index(inplace = True)\n",
    "event_tweet_vec = tweet_tfidf_model_fit.transform(event_tweet.cleaned_tweet.values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(873, 7) (873, 129380) (410831, 130567)\n"
     ]
    }
   ],
   "source": [
    "print(event_tweet.shape, event_tweet_vec.shape, tweet_tfidf_transform.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<410831x130567 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2353731 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_tfidf_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SVD = TruncatedSVD(n_components = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'tweet_tfidf_transform',\n",
       " b'tweet_tfidf_model',\n",
       " b'tweet_tfidf_model_fit',\n",
       " b'tweet_SVD_model_fit',\n",
       " b'SVD_model_fit']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVD_model_fit = SVD.fit(tweet_tfidf_transform)\n",
    "r = redis.StrictRedis(redis_ip)\n",
    "LSA = pickle.dumps(SVD_model_fit)\n",
    "r.set('tweet_SVD_model_fit', SVD_model_fit)\n",
    "r.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x130567 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 4 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_tfidf_transform[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): gensim in /opt/conda/lib/python3.5/site-packages\n",
      "Requirement already satisfied (use --upgrade to upgrade): numpy>=1.3 in /opt/conda/lib/python3.5/site-packages (from gensim)\n",
      "Requirement already satisfied (use --upgrade to upgrade): scipy>=0.7.0 in /opt/conda/lib/python3.5/site-packages (from gensim)\n",
      "Requirement already satisfied (use --upgrade to upgrade): six>=1.5.0 in /opt/conda/lib/python3.5/site-packages (from gensim)\n",
      "Requirement already satisfied (use --upgrade to upgrade): smart-open>=1.2.1 in /opt/conda/lib/python3.5/site-packages (from gensim)\n",
      "Requirement already satisfied (use --upgrade to upgrade): bz2file in /opt/conda/lib/python3.5/site-packages (from smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied (use --upgrade to upgrade): boto>=2.32 in /opt/conda/lib/python3.5/site-packages (from smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied (use --upgrade to upgrade): requests in /opt/conda/lib/python3.5/site-packages (from smart-open>=1.2.1->gensim)\n",
      "\u001b[33mYou are using pip version 8.1.2, however version 9.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-07 02:44:02,477 : INFO : collecting all words and their counts\n",
      "2017-06-07 02:44:02,478 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-06-07 02:44:02,479 : INFO : collected 3 word types from a corpus of 4 raw words and 2 sentences\n",
      "2017-06-07 02:44:02,480 : INFO : Loading a fresh vocabulary\n",
      "2017-06-07 02:44:02,481 : INFO : min_count=1 retains 3 unique words (100% of original 3, drops 0)\n",
      "2017-06-07 02:44:02,481 : INFO : min_count=1 leaves 4 word corpus (100% of original 4, drops 0)\n",
      "2017-06-07 02:44:02,482 : INFO : deleting the raw counts dictionary of 3 items\n",
      "2017-06-07 02:44:02,483 : INFO : sample=0.001 downsamples 3 most-common words\n",
      "2017-06-07 02:44:02,484 : INFO : downsampling leaves estimated 0 word corpus (5.7% of prior 4)\n",
      "2017-06-07 02:44:02,485 : INFO : estimated required memory for 3 words and 100 dimensions: 3900 bytes\n",
      "2017-06-07 02:44:02,485 : INFO : resetting layer weights\n",
      "2017-06-07 02:44:02,486 : INFO : training model with 3 workers on 3 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-06-07 02:44:02,488 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-06-07 02:44:02,489 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-06-07 02:44:02,490 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-06-07 02:44:02,491 : INFO : training on 20 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2017-06-07 02:44:02,492 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "# import modules & set up logging\n",
    "!pip install gensim\n",
    "import gensim, logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "sentences = [['first', 'sentence'], ['second', 'sentence']]\n",
    "# train word2vec on the two sentences\n",
    "model = gensim.models.Word2Vec(sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MySentences(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    " \n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            for line in open(os.path.join(self.dirname, fname)):\n",
    "                yield line.split()\n",
    " \n",
    "    sentences = MySentences('/some/directory') # a memory-friendly iterator\n",
    "    model = gensim.models.Word2Vec(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences, min_count=10)  # default value is 5\n",
    "model = Word2Vec(sentences, size=200)  # default value is 100\n",
    "model = Word2Vec(sentences, workers=4) # default = 1 worker = no parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tweets_event_ab_test(event, n = 200):\n",
    "    event_tweet = df[df['cleaned_tweet'].str.contains((event))]\n",
    "    event_tweet.reset_index(inplace = True)\n",
    "    \n",
    "    A_index = list(range(event_tweet.shape[0]))\n",
    "    random.shuffle(A_index)\n",
    "    \n",
    "    n = n\n",
    "    A1_index = []\n",
    "    for i in range(n):\n",
    "        A1_index.append(A_index.pop())\n",
    "    \n",
    "    A1 = event_tweet.iloc[A1_index,:]\n",
    "    A1.reset_index(inplace = True)\n",
    "    A1_vec = np.array([tweet_tfidf_model.transform(i).vector for i in A1['cleaned_tweet']])\n",
    "    \n",
    "    A2 = event_tweet.iloc[A_index,:] \n",
    "    A2.reset_index(inplace = True)\n",
    "    A2_vec = np.array([nlp(i).vector for i in A2['cleaned_tweet']])\n",
    "    non_event_tweet = df[~df['cleaned_tweet'].str.contains(('event'))]\n",
    "    non_event_tweet.reset_index(inplace = True)\n",
    "    \n",
    "    B_index = list(range(non_event_tweet.shape[0]))\n",
    "    random.shuffle(B_index)\n",
    "    n = n\n",
    "    B1_index = []\n",
    "    for i in range(n):\n",
    "        B1_index.append(B_index.pop())\n",
    "    m = A2.shape[0]\n",
    "    B2_index = []\n",
    "    for i in range(m):\n",
    "        B2_index.append(B_index.pop())  \n",
    "    \n",
    "    \n",
    "    B1 = non_event_tweet.iloc[B1_index,:]\n",
    "    B1.reset_index(inplace = True)\n",
    "    B1_vec = np.array([nlp(i).vector for i in B1['cleaned_tweet']])\n",
    "    \n",
    "    B2 = non_event_tweet.iloc[B2_index,:]\n",
    "    B2.reset_index(inplace = True)\n",
    "    B2_vec = np.array([nlp(i).vector for i in B2['cleaned_tweet']])\n",
    "    \n",
    "    A1_vec_mean = np.average(A1_vec, axis=0)\n",
    "    A2_vec_mean = np.average(A2_vec, axis=0)\n",
    "    B1_vec_mean = np.average(B1_vec, axis=0)\n",
    "    B2_vec_mean = np.average(B2_vec, axis=0)\n",
    "    \n",
    "    a1a2 = cosine_similarity(A1_vec_mean.reshape(1,-1),A2_vec_mean.reshape(1,-1))[0][0]\n",
    "    b1b2 = cosine_similarity(B2_vec_mean.reshape(1,-1),B1_vec_mean.reshape(1,-1))[0][0]\n",
    "    a1b1 = cosine_similarity(A1_vec_mean.reshape(1,-1),B1_vec_mean.reshape(1,-1))[0][0]\n",
    "    a2b2 = cosine_similarity(A2_vec_mean.reshape(1,-1),B2_vec_mean.reshape(1,-1))[0][0]\n",
    "   \n",
    "    a2_consim_list = []\n",
    "    for i in range(min(n,m)):  \n",
    "        a2_consim_list.append(cosine_similarity(A2_vec[i].reshape(1,-1),A1_vec_mean.reshape(1,-1))[0][0])\n",
    "    a2_mean = np.mean(np.array(a2_consim_list))\n",
    "    a2_std = np.std(np.array(a2_consim_list))\n",
    "    \n",
    "    b2_consim_list = []\n",
    "    for i in range(min(n,m)):  \n",
    "        b2_consim_list.append(cosine_similarity(B2_vec[i].reshape(1,-1),B1_vec_mean.reshape(1,-1))[0][0])\n",
    "    b2_mean = np.mean(b2_consim_list)\n",
    "    b2_std = np.std(b2_consim_list)\n",
    "    \n",
    "    a1b2_consim_list = []\n",
    "    for i in range(min(n,m)):  \n",
    "        a1b2_consim_list.append((cosine_similarity(B2_vec[i].reshape(1,-1),A1_vec_mean.reshape(1,-1))[0][0]))\n",
    "    a1b2_mean = np.mean(a1b2_consim_list)\n",
    "    a1b2_std = np.std(a1b2_consim_list)\n",
    "    \n",
    "    print('A1|A2: ',a1a2,'\\n'\n",
    "                'B1|B2: ',b1b2, '\\n\\n'\n",
    "                'A1|B1: ',a1b1, '\\n'\n",
    "                'A2|B2: ', a2b2, '\\n\\n'\n",
    "                'Cosine Similarity Mean of A2 to A1', a2_mean, '\\n'\n",
    "                'Cosine Similarity Mean of B2 to B1', b2_mean, '\\n'\n",
    "                'Cosine Similarity Mean of B2 to A1', a1b2_mean, '\\n\\n'\n",
    "                'Cosine Similarity STD of A2 to A1', a2_std, '\\n'\n",
    "                'Cosine Similarity STD of B2 to B1', b2_std, '\\n'\n",
    "                'Cosine Similarity STD of B2 to A1', a1b2_std, '\\n'                 \n",
    "                )\n",
    "    \n",
    "    return a2_consim_list, b2_consim_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LSA.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a2_consim_list, b2_consim_list = tweets_event_ab_test('paris|climate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(range(len(a2_consim_list)), a2_consim_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(range(len(b2_consim_list)), b2_consim_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def event_tweet_count(event):\n",
    "    event_tweet = df[df['cleaned_tweet'].str.contains((event))]\n",
    "    return len(event_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "event_tweet_count('nbafinal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets_event_ab_test('nbafinal', n = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_event_ab_test('travel ban', n = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Top_tweets_in_b(event, n = 200):\n",
    "    A = df[df['cleaned_tweet'].str.contains((event))]\n",
    "    A.reset_index(inplace = True)\n",
    "    A_vec = np.array([nlp(i).vector for i in A['cleaned_tweet']])\n",
    "    A_vec_mean = np.average(A_vec, axis=0)\n",
    "    \n",
    "    \n",
    "    B = df[~df['cleaned_tweet'].str.contains(('event'))]\n",
    "    B.reset_index(inplace = True)\n",
    "    \n",
    "    B_index = list(range(B.shape[0]))\n",
    "    random.shuffle(B_index)\n",
    "    n = n\n",
    "    B1_index = []\n",
    "    for i in range(n):\n",
    "        B1_index.append(B_index.pop())\n",
    "   \n",
    "        \n",
    "    B1 = B.iloc[B1_index,:]\n",
    "    B1.reset_index(inplace = True)\n",
    "    B1_vec = np.array([nlp(i).vector for i in B1['cleaned_tweet']])\n",
    "    \n",
    "    \n",
    "    consim_twt_list = []\n",
    "    for i in range(n):  \n",
    "        consim_twt_list.append((cosine_similarity(B1_vec[i].reshape(1,-1),A_vec_mean.reshape(1,-1))[0][0], B1['cleaned_tweet'][i]))\n",
    "    pd.options.display.max_colwidth = 200\n",
    "    result = pd.DataFrame(consim_twt_list, columns = ['score','tweet'])\n",
    "    print(result.shape)\n",
    "    result_90 = result[result['score']>.90]\n",
    "    result_90.sort_values('score', axis = 0, ascending = False)\n",
    "    return result_90['tweet'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Top_tweets_in_b('paris|climate', n = 30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "event_tweet_count('nationaldonutday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Top_tweets_in_b('nationaldonutday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_event_ab_test('nationaldonutday', n = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
