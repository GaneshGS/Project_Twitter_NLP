{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import random\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from dateutil import tz\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "#!pip install redis\n",
    "import redis\n",
    "redis_ip = '34.211.59.66'\n",
    "r = redis.StrictRedis(redis_ip)\n",
    "#!pip install wordcloud\n",
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2 as pg2\n",
    "import psycopg2.extras as pgex\n",
    "this_host='34.211.59.66'\n",
    "this_user='postgres'\n",
    "this_password='postgres'\n",
    "conn = pg2.connect(host = this_host, \n",
    "                        user = this_user,\n",
    "                        password = this_password)\n",
    "\n",
    "sql_select = '''select created_at, location, tweet_content, cleaned_tweet, hashtags, date, time, date_time  \n",
    "                from tweets where (date_time >= NOW() - '18 hour'::INTERVAL) and hashtags is not null and hashtags != 'None';'''\n",
    "\n",
    "cur = conn.cursor(cursor_factory=pgex.RealDictCursor)\n",
    "cur.execute(sql_select)\n",
    "rows = cur.fetchall()\n",
    "conn.close()\n",
    "df = pd.DataFrame(rows)\n",
    "df.reset_index(inplace = True)\n",
    "df['created_datetime'] = pd.to_datetime(df['created_at'])\n",
    "df['year'] = df.created_datetime.apply(lambda x: x.year)\n",
    "df['month'] = df.created_datetime.apply(lambda x: x.month)\n",
    "df['day'] = df.created_datetime.apply(lambda x: x.day)\n",
    "df['hour'] = df.created_datetime.apply(lambda x: x.hour)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df[~df['hashtags'].str.contains('amp')]\n",
    "df = df[~df['hashtags'].str.contains('job|hiring|sales|retail|clerical')]\n",
    "df = df[~df['hashtags'].str.contains('dallas|plano|irving|richardson|garland|allen|odessa|midland|sanfrancisco|losangeles|irvine|garland')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[~df['location'].str.contains('.ca|california')][~df['location'].str.contains('.tx|texas')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ca_df = df[df['location'].str.contains('.ca|california')]\n",
    "tx_df = df[df['location'].str.contains('.tx|texas')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_cloud(state = None): \n",
    "    state = state.lower()\n",
    "    state_df = df[df['location'].str.contains('.{}'.format(state))]\n",
    "    words = state_df['hashtags'][(state_df['hashtags'].isnull() == False) & (state_df['hashtags'] != 'None')]\n",
    "    stopwords = set(STOPWORDS)\n",
    "    #stopwords.add(\"amp\")\n",
    "    wc = WordCloud(width=1600, height=800, background_color='white', \\\n",
    "                   relative_scaling=1, stopwords=stopwords, colormap='copper').generate(' '.join(i for i in words))\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.imshow(wc)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cloud('TX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cloud('CA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hastages_series = df['hashtags']\n",
    "len(hastages_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(min_df = 1, stop_words='english')\n",
    "hashtags_countvec = count_vectorizer.fit_transform(hastages_series)\n",
    "#hashtags_countvec = pickle.dumps(hashtags_countvec_fit)\n",
    "#r.set('hashtags_countvec_fit_temp', hashtags_countvec)\n",
    "#hashtags_countvec = pickle.loads(r.get('hashtags_countvec_fit_temp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_delta = max(df['created_datetime']) - min(df['created_datetime'])\n",
    "time_window = time_delta.components.days*24 + time_delta.components.hours\n",
    "time_lag = timedelta(hours = .25)\n",
    "time_gap = timedelta(hours = 0.1)\n",
    "windows = int(round(time_delta/time_gap,0))\n",
    "print('start time: ', min(df['created_datetime']),'\\n',\\\n",
    "      'end time:  ', max(df['created_datetime']),'\\n',\\\n",
    "      'total hours: ',time_window,'\\n',\\\n",
    "      'time lag: ', time_lag,'\\n',\\\n",
    "      'time gap: ', time_gap,'\\n',\\\n",
    "      'time windows: ', windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def group_consecutives(vals, step=1):\n",
    "    \"\"\"Return list of consecutive lists of numbers from vals (number list).\"\"\"\n",
    "    run = []\n",
    "    result = [run]\n",
    "    expect = None\n",
    "    for v in vals:\n",
    "        if (v == expect) or (expect is None):\n",
    "            run.append(v)\n",
    "        else:\n",
    "            run = [v]\n",
    "            result.append(run)\n",
    "        expect = v + step\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_trend_period(trend_group):        \n",
    "    new_result = []\n",
    "    count = 0\n",
    "    for i in range(len(trend_group)-1):\n",
    "        if (count > len(trend_group)-2):\n",
    "            pass\n",
    "        else:\n",
    "            gap = ((trend_group[count+1][0]) - (trend_group[count][len(trend_group[count])-1]))\n",
    "            if gap <.8*min((trend_group[count+1][0]), (trend_group[count][len(trend_group[count])-1])):\n",
    "                new_period = trend_group[count]+trend_group[count+1]\n",
    "                new_result.append(new_period)\n",
    "                count +=2\n",
    "            else:\n",
    "                new_result.append(trend_group[count])\n",
    "                count +=1\n",
    "    return new_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time_delta = max(df['created_datetime']) - min(df['created_datetime'])\n",
    "time_window = time_delta.components.days*24 + time_delta.components.hours\n",
    "time_lag = timedelta(hours = .3)\n",
    "time_gap = timedelta(hours = .1)\n",
    "windows = int(round(time_delta/time_gap,0))\n",
    "time_format = '%Y-%h-%d %a %I %p'\n",
    "from_zone = tz.tzutc()\n",
    "#to_zone = tz.tzlocal()\n",
    "to_zone = 'US/Pacific'\n",
    "def hashtag_trend(hashtag):\n",
    "    hashtag = hashtag.lower()\n",
    "    arr = []\n",
    "    arr_all = []\n",
    "    start_time = min(df['created_datetime'])\n",
    "    for window in range(windows):\n",
    "        start_time = start_time\n",
    "        end_time = start_time + time_lag\n",
    "        subset = df['hashtags'][((df['created_datetime'] < end_time) & (df['created_datetime'] > start_time))]\n",
    "        h = subset.str.contains(hashtag).mean()\n",
    "        arr.append(h)\n",
    "        #arr_all.append(subset.mean())\n",
    "        start_time += time_gap\n",
    "    w = np.array(range(windows))\n",
    "    timeline = []\n",
    "    for i in w:\n",
    "        timeline.append(min(df['created_datetime']) + time_gap * i) \n",
    "    arr = np.array(arr)  \n",
    "    #arr_all = np.array(arr_all) \n",
    "    grad = np.gradient(arr)\n",
    "    #grad2 = np.gradient(grad)\n",
    "    #tr = np.argwhere(grad>.01).reshape(1,-1)[0]\n",
    "    time_df = pd.DataFrame({'arr':arr,'grad':grad},index=timeline)\n",
    "    rolmean_arr = pd.rolling_mean(arr, window=12)\n",
    "    rolstd_arr = pd.rolling_std(arr, window=12)\n",
    "    rolmean_grad = np.gradient(rolmean_arr)\n",
    "    spike = []\n",
    "    trend = np.argwhere(rolmean_grad>0.001).reshape(1,-1)[0]\n",
    "    trend_group = group_consecutives(trend)\n",
    "        \n",
    "    trend_group1 = new_trend_period(trend_group)\n",
    "    #if trend_group1 != trend_group:\n",
    "    #    trend_group1 = new_trend_period(trend_group1)\n",
    "        \n",
    "    trend_group2 = [i for i in trend_group1 if (len(i) > 7)]\n",
    "    spike.append([[i[0],i[len(i)-1]] for i in trend_group2])\n",
    "    spike = np.array(spike)\n",
    "    spk1 = [i[0] for i in spike[0]]\n",
    "    spk2 = [i[1] for i in spike[0]]\n",
    "        \n",
    "    plt.figure(figsize=(14,7))\n",
    "    plt.plot(arr, label='Hashtag Frequency',c = 'navy')\n",
    "    plt.plot(rolmean_arr, label='Rolling Means',c = 'lightblue')\n",
    "    plt.plot(rolstd_arr, label='Rolling STDs',c = 'gray')\n",
    "    plt.plot(rolmean_grad, label='Gradient over Rolling Means',c = 'red')\n",
    "    #plt.plot(grad2, label='grad2 Slope')\n",
    "    for j,k in zip(spk1,spk2):\n",
    "        spike1 = (min(df['created_datetime']) + time_gap * j) \n",
    "        spike1 = spike1.replace(tzinfo=from_zone)\n",
    "        spike1_et = spike1.astimezone(to_zone)\n",
    "        spike1_et = spike1_et.strftime(time_format)\n",
    "       \n",
    "        spike2 = (min(df['created_datetime']) + time_gap * k) \n",
    "        spike2 = spike2.replace(tzinfo=from_zone)\n",
    "        spike2_et = spike2.astimezone(to_zone)\n",
    "        spike2_et = spike2_et.strftime(time_format)           \n",
    "        plt.axvline(j, color = 'salmon',linestyle='dashed', label = 'Trending: {}'.format(spike1_et))\n",
    "        plt.axvspan(j, k, alpha=0.2, color='lightcoral')\n",
    "    plt.title(hashtag, fontsize=20)    \n",
    "    plt.legend(fontsize=12) \n",
    "    ### -----ARIMA MODELING\n",
    "#     from statsmodels.tsa.arima_model import ARIMA\n",
    "#     w = np.array(range(windows))\n",
    "#     timeline = []\n",
    "#     for i in w:\n",
    "#         timeline.append(min(df['created_datetime']) + time_gap * i) \n",
    "#     arima_df = pd.DataFrame({'perf':arr},index=timeline)   \n",
    "\n",
    "#     series = arima_df\n",
    "#     model = ARIMA(series, order=(5,1,0))\n",
    "#     model_fit = model.fit()\n",
    "#     print(model_fit.summary())\n",
    "#     # plot residual errors\n",
    "#     residuals =pd. DataFrame(model_fit.resid)\n",
    "#     residuals.plot()\n",
    "#     plt.show()\n",
    "#     residuals.plot(kind='kde')\n",
    "#     plt.show()\n",
    "#     print(residuals.describe())\n",
    "    ###-----------------\n",
    "    return  time_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hashtag_freq_df = pd.DataFrame({'hashtag': count_vectorizer.get_feature_names(), 'occurrences':np.asarray(hashtags_countvec.sum(axis=0)).ravel().tolist()})\n",
    "hashtag_freq_df['frequency'] = hashtag_freq_df['occurrences']/np.sum(hashtag_freq_df['occurrences'])\n",
    "hashtag_freq_df.sort_values(by = 'occurrences',ascending = False, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_freq_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "a = [hashtag_trend(i) for i in (hashtag_freq_df['hashtag'].head(10))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "time_df = hashtag_trend('pride2017')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "def svd_variance(svd_model, col_index):\n",
    "    \n",
    "    dimensions = ['Dimension {}'.format(i) for i in range(1,len(svd_model.components_)+1)]\n",
    "    components = pd.DataFrame(np.round(svd_model.components_, 4), columns=col_index)\n",
    "    ratios = svd_model.explained_variance_ratio_.reshape(len(svd_model.components_), 1)\n",
    "    variance_ratios = pd.DataFrame(np.round(ratios, 4), columns = ['Explained Variance'])\n",
    "    variance_ratios.index = dimensions\n",
    "\n",
    "    fig, ax = plt.subplots(figsize = (24,10))\n",
    "\n",
    "    # Plot the feature weights as a function of the components\n",
    "    components.plot(ax = ax, kind = 'bar',legend='False');\n",
    "    ax.set_ylabel(\"Feature Weights\")\n",
    "    ax.set_xticklabels(dimensions, rotation=0)\n",
    "    ax.legend_.remove()\n",
    "\n",
    "\n",
    "    # Display the explained variance ratios\n",
    "    for i, ev in enumerate(svd_model.explained_variance_ratio_):\n",
    "        ax.text(i-0.40, ax.get_ylim()[1] + 0.05, \"Explained Variance\\n          %.4f\"%(ev))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "svd_variance(svd, tfd_word_index)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cv_svd_pipe = Pipeline([\n",
    "    ('cv',CountVectorizer(min_df = 1, stop_words='english')),\n",
    "    ('svd',TruncatedSVD(n_components=25))\n",
    "])\n",
    "cv = cv_svd_pipe.steps[0][1]\n",
    "svd = cv_svd_pipe.steps[1][1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
